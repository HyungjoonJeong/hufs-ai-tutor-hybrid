import streamlit as st
import os
import tempfile
import queue
import time
import concurrent.futures # íŒŒì¼ ìµœìƒë‹¨ì— ì¶”ê°€í•´ì£¼ì„¸ìš”!
from dotenv import load_dotenv

# 1. í•µì‹¬ ì„¤ê³„ë„ (Core)
from langchain_core.prompts import PromptTemplate

# 2. êµ¬ê¸€ AI ì—°ê²° (Google GenAI)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


# 3. ë°ì´í„°ë² ì´ìŠ¤ (Community)
from langchain_community.vectorstores import FAISS

# 4. ë©”ëª¨ë¦¬ (ê°€ì¥ ì•ˆì „í•œ ìµœì‹  ê²½ë¡œë¡œ ë³€ê²½)
# ë§Œì•½ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ from langchain_community.chat_message_histories import ... ë¡œ ì„ íšŒí•´ì•¼ í•©ë‹ˆë‹¤.
#try:
#    from langchain.memory import ConversationBufferMemory
#except ImportError:
#    from langchain_community.memory import ConversationBufferMemory

# 5. ê²€ìƒ‰ ì—”ì§„ (Classic)
from langchain_classic.chains import RetrievalQA

# 6. ì§ì ‘ ë§Œë“  ëª¨ë“ˆ
from extract_text import extract_documents_from_pdf, split_documents

# --------------------------------
# ê¸°ë³¸ ì„¤ì •
# --------------------------------
load_dotenv()

st.set_page_config(
    page_title="HUFS AI Tutor",
    layout="wide"
)

st.title("HUFS RAG ê¸°ë°˜ AI íŠœí„° (GPT-5.2 & Gemini 2.5)")
st.caption("ê°•ì˜ ìë£Œ ê¸°ë°˜ìœ¼ë¡œ GPTì™€ Geminië¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ë©° ì¶œì²˜ë¥¼ ëª…í™•íˆ ì œì‹œí•©ë‹ˆë‹¤.")

# --------------------------------
# ì„¸ì…˜ ìƒíƒœ
# --------------------------------
# --- app.py ìƒë‹¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë¶€ë¶„ ---

if "gpt_messages" not in st.session_state:
    st.session_state.gpt_messages = []

if "gemini_messages" not in st.session_state:
    st.session_state.gemini_messages = []

# ê¸°ì¡´ messagesëŠ” ë” ì´ìƒ ì“°ì§€ ì•Šì§€ë§Œ, 
# í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‚¨ê²¨ë‘ê±°ë‚˜ ì•„ë˜ì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
# if "messages" not in st.session_state:
#     st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# --------------------------------
# ì§ˆë¬¸ ë¶„ë¥˜ê¸°
# --------------------------------
def classify_question(question: str) -> str:
    # í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” ì„¤ì •ì´ ë³µì¡í•œ Gemini ëŒ€ì‹  GPT-4o-minië¥¼ ì”ë‹ˆë‹¤. (ë§¤ìš° ì €ë ´)
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì„ 'concept', 'calculation', 'summary' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´. í•œ ë‹¨ì–´ë§Œ ë‹µí•´. ì§ˆë¬¸: {question}"
    result = llm.invoke(prompt)
    return result.content.strip().lower()


# --------------------------------
# --------------------------------
# ê³„ì‚° ë¬¸ì œ ì „ìš© ì²´ì¸ (GPT/Gemini ëŒ€ì‘)
# --------------------------------
def run_calculation_chain(question: str, model_type: str, vector_db):
    # 1. ëª¨ë¸ ì„ íƒ
    if model_type == "gpt":
        llm = ChatOpenAI(model="gpt-5.2", temperature=0)
    else:
        # 2026ë…„ ê¸°ì¤€ ìµœì‹  ì•ˆì • ë²„ì „ì¸ 1.5-flash ê¶Œì¥
        llm = ChatGoogleGenerativeAI(model="gemini-2.5", temperature=0)

    # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
# í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ st.session_state.vector_db ëŒ€ì‹  vector_db ì‚¬ìš©!
    docs = vector_db.similarity_search(question, k=7)
    context = "\n\n".join([d.page_content for d in docs])

    template = """
ë„ˆëŠ” ëŒ€í•™ ê³¼ëª© ê³„ì‚° ë¬¸ì œë¥¼ í‘¸ëŠ” ì¡°êµì´ë‹¤. ì œê³µëœ [ë¬¸ë§¥]ì˜ ê³µì‹ê³¼ ìˆ˜ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ì–´ë¼.

[ìˆ˜ì‹ ì‘ì„± ê·œì¹™ - ì¤‘ìš”]
1. ëª¨ë“  ìˆ˜í•™ ê³µì‹ì´ë‚˜ ë³€ìˆ˜ëŠ” ë°˜ë“œì‹œ LaTeX í˜•ì‹ì„ ì‚¬ìš©í•˜ë¼.
2. ë¬¸ì¥ ì•ˆì˜ ì§§ì€ ìˆ˜ì‹ì€ $ê¸°í˜¸ í•˜ë‚˜ë¡œ ê°ì‹¸ë¼. (ì˜ˆ: $P(Z < z)$)
3. ë³„ë„ì˜ ì¤„ì— í‘œì‹œí•´ì•¼ í•˜ëŠ” ë³µì¡í•œ ê³µì‹ì€ $$ê¸°í˜¸ ë‘ ê°œë¡œ ê°ì‹¸ë¼.
(ì˜ˆ: $$f(x) = \\frac{{1}}{{\\sigma\\sqrt{{2\\pi}}}} \exp...$$)
4. ì ˆëŒ€ [ ] ë‚˜ ( ) ë¡œ ìˆ˜ì‹ì„ ê°ì‹¸ì§€ ë§ˆë¼. ì˜¤ì§ $ì™€ $$ë§Œ ì‚¬ìš©í•œë‹¤.

[í‘œ ì‘ì„± ê·œì¹™ - ì¤‘ìš”]
1. ë°ì´í„°ë¥¼ ë¹„êµí•˜ê±°ë‚˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¼ ë•ŒëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ(Table) í˜•ì‹ì„ ì‚¬ìš©í•˜ë¼.
2. í‘œì˜ í—¤ë”ì™€ ë°ì´í„° ì‚¬ì´ì—ëŠ” ë°˜ë“œì‹œ êµ¬ë¶„ì„ (---)ì„ ë„£ì–´ë¼.
3. ì²« ë²ˆì§¸ ì—´ì˜ ì œëª©ì— íŠ¹ìˆ˜ê¸°í˜¸(|)ë¥¼ ë‹¨ë…ìœ¼ë¡œ ì“°ì§€ ë§ê³ , 'êµ¬ë¶„' ë˜ëŠ” 'Y \ X' ì²˜ëŸ¼ í…ìŠ¤íŠ¸ë¡œ ëª…í™•íˆ ì ì–´ë¼.
   (ì˜ˆ: | Y \ X | 0 | 1 | 2 |)
4. ì¤‘ê°„ì— ì„¤ëª…ì„ ë„£ê¸° ìœ„í•´ í‘œë¥¼ ëŠì§€ ë§ê³ , í‘œë¥¼ ë¨¼ì € ì™„ë²½íˆ ì™„ì„±í•œ í›„ ì„¤ëª…ì„ ë§ë¶™ì—¬ë¼.

[ê·œì¹™]
1. í’€ì´ ê³¼ì •ì„ ê°€ë…ì„±ì„ ìœ„í•´ ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ë°˜ë“œì‹œ ìˆ«ì ì¸ë±ìŠ¤(1., 2., 3.)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ì—¬ ìƒì„¸íˆ ì„¤ëª…í•˜ë¼.
2. ìˆ˜ì‹ì€ LaTeX í˜•ì‹ì´ë‚˜ ëª…í™•í•œ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œì‹œí•˜ë¼.
3. ë§ˆì§€ë§‰ì— ìµœì¢… ë‹µì„ 'ì •ë‹µ: 'ê³¼ í•¨ê»˜ ì •ë¦¬í•˜ë¼.
4. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ê°€ê¸‰ì  ì‚¬ìš©í•˜ì§€ ë§ê³ , ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ë¼ê³  ì•ˆë‚´í•˜ë¼.
2. ê° í¬ì¸íŠ¸ë§ˆë‹¤ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë³¼ë“œì²´(**)ë¡œ í‘œì‹œí•˜ë¼.
3. ê°•ì˜ ìë£Œì˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë˜, ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ë¼.
5. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
6. ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œì™€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ë¼.
7. ë‹µë³€ì€ ìµœì†Œ 3ë¬¸ë‹¨ ì´ìƒì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.
8. ê°•ì˜ ìë£Œì— ìˆëŠ” ì˜ˆì‹œë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•  ê²ƒ.
9. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµì„ ë•ê¸° ìœ„í•´ 'ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê°œë…'ì„ ë‘ì„¸ ë¬¸ì¥ ë§ë¶™ì¼ ê²ƒ.

[ë¬¸ë§¥]
{context}

[ë¬¸ì œ]
{question}

[í’€ì´]
"""
    prompt = template.format(
        length_instruction=length_instruction,
        chat_history=chat_history_str,
        context=context_text,
        question=question
    )

    # 3. ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
    try:
        for chunk in llm.stream(prompt):
            if hasattr(chunk, 'content'):
                content = chunk.content
            else:
                content = str(chunk)
            
            if content:
                yield content
    except Exception as e:
        yield f"\nâš ï¸ ëª¨ë¸({model_type}) í˜¸ì¶œ ì—ëŸ¬: {str(e)}"


# --------------------------------
# ì¼ë°˜ RAG ì²´ì¸
# --------------------------------
# 2. ë©”ì¸ ë‹µë³€ ì²´ì¸ (GPT-4o ì‚¬ìš© - ì •ë°€í•œ ë…¼ë¦¬)
# run_rag ì •ì˜ ë¶€ë¶„ ìˆ˜ì •
def run_rag_stream(question: str, answer_style: str, model_type: str, chat_history: list, docs: list):
    try:
        # 1. ëª¨ë¸ ì„¤ì • (ìµœì‹  ëª¨ë¸ëª… ë°˜ì˜)
        if model_type == "gpt":
            llm = ChatOpenAI(model="gpt-5.2", temperature=0.7, streaming=True)
        else:
            llm = ChatGoogleGenerativeAI(model="gemini-2.5", temperature=0.7, streaming=True)

        # 2. ì»¨í…ìŠ¤íŠ¸ ë° íˆìŠ¤í† ë¦¬ êµ¬ì„±
        context_text = "\n\n".join([d.page_content for d in docs])
        chat_history_str = "\n".join([f"{m['role']}: {m['content']}" for m in (chat_history or [])])
        
        length_instruction = (
            "í•µì‹¬ ìœ„ì£¼ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ê°„ê²°í•˜ê²Œ ë‹µí•˜ë¼." if answer_style == "ì§§ê²Œ" 
            else "ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ìì„¸íˆ ë‹µí•˜ë¼."
        )

        template = """

ë‹¹ì‹ ì€ í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµì˜ 1íƒ€ ê°•ì‚¬ AI íŠœí„°ì…ë‹ˆë‹¤. 
ì œê³µëœ [ê°•ì˜ ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ìˆ˜ì‹ ì‘ì„± ê·œì¹™ - ì¤‘ìš”]
1. ëª¨ë“  ìˆ˜í•™ ê³µì‹ì´ë‚˜ ë³€ìˆ˜ëŠ” ë°˜ë“œì‹œ LaTeX í˜•ì‹ì„ ì‚¬ìš©í•˜ë¼.
2. ë¬¸ì¥ ì•ˆì˜ ì§§ì€ ìˆ˜ì‹ì€ $ê¸°í˜¸ í•˜ë‚˜ë¡œ ê°ì‹¸ë¼. (ì˜ˆ: $P(Z < z)$)
3. ë³„ë„ì˜ ì¤„ì— í‘œì‹œí•´ì•¼ í•˜ëŠ” ë³µì¡í•œ ê³µì‹ì€ $$ê¸°í˜¸ ë‘ ê°œë¡œ ê°ì‹¸ë¼.
(ì˜ˆ: $$f(x) = \\frac{{1}}{{\\sigma\\sqrt{{2\\pi}}}} \exp...$$)
4. ì ˆëŒ€ [ ] ë‚˜ ( ) ë¡œ ìˆ˜ì‹ì„ ê°ì‹¸ì§€ ë§ˆë¼. ì˜¤ì§ $ì™€ $$ë§Œ ì‚¬ìš©í•œë‹¤.

[í‘œ ì‘ì„± ê·œì¹™ - ì¤‘ìš”]
1. ë°ì´í„°ë¥¼ ë¹„êµí•˜ê±°ë‚˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¼ ë•ŒëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ(Table) í˜•ì‹ì„ ì‚¬ìš©í•˜ë¼.
2. í‘œì˜ í—¤ë”ì™€ ë°ì´í„° ì‚¬ì´ì—ëŠ” ë°˜ë“œì‹œ êµ¬ë¶„ì„ (---)ì„ ë„£ì–´ë¼.
3. ì²« ë²ˆì§¸ ì—´ì˜ ì œëª©ì— íŠ¹ìˆ˜ê¸°í˜¸(|)ë¥¼ ë‹¨ë…ìœ¼ë¡œ ì“°ì§€ ë§ê³ , 'êµ¬ë¶„' ë˜ëŠ” 'Y \ X' ì²˜ëŸ¼ í…ìŠ¤íŠ¸ë¡œ ëª…í™•íˆ ì ì–´ë¼.
   (ì˜ˆ: | Y \ X | 0 | 1 | 2 |)
4. ì¤‘ê°„ì— ì„¤ëª…ì„ ë„£ê¸° ìœ„í•´ í‘œë¥¼ ëŠì§€ ë§ê³ , í‘œë¥¼ ë¨¼ì € ì™„ë²½íˆ ì™„ì„±í•œ í›„ ì„¤ëª…ì„ ë§ë¶™ì—¬ë¼.

[ê·œì¹™]
1. ê°€ë…ì„±ì„ ìœ„í•´ ë°˜ë“œì‹œ ìˆ«ì ì¸ë±ìŠ¤(1., 2., 3.)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ë¼.
2. ê° í¬ì¸íŠ¸ë§ˆë‹¤ í•µì‹¬ ì œëª©ì„ ë³¼ë“œì²´(**)ë¡œ í‘œì‹œí•˜ë¼.
3. ê°•ì˜ ìë£Œì˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë˜, ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ë¼.
4. ë°˜ë“œì‹œ ë¬¸ë§¥ì— ê·¼ê±°í•´ ë‹µí•˜ë¼.
5. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
6. ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œì™€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ë¼.
7. ë‹µë³€ì€ ìµœì†Œ 3ë¬¸ë‹¨ ì´ìƒì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.
8. ê°•ì˜ ìë£Œì— ìˆëŠ” ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•  ê²ƒ.
9. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµì„ ë•ê¸° ìœ„í•´ 'ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê°œë…'ì„ ë‘ì„¸ ë¬¸ì¥ ë§ë¶™ì¼ ê²ƒ.

[ë¬¸ë§¥]
{context}

[ì§ˆë¬¸]
{question}

ë‹µë³€:"""
        # .format ì¸ì ì •í™•íˆ ë§¤ì¹­
        formatted_prompt = template.format(
            context=context_text,
            question=question
        )

        # 3. ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
        for chunk in llm.stream(formatted_prompt):
            content = chunk.content if hasattr(chunk, 'content') else str(chunk)
            if content:
                yield content

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ yieldë¡œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í™”ë©´ì— ì¶œë ¥
        yield f"\n\nâŒ [ëª¨ë¸ ì—ëŸ¬] {model_type}: {str(e)}"



# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
with st.sidebar:
    st.header("ì„¤ì •")

    answer_style = st.radio(
        "ë‹µë³€ ê¸¸ì´",
        ["ì§§ê²Œ", "ìì„¸íˆ"],
        index=1
    )

    # ì‚¬ì´ë“œë°”ì˜ ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ë¶€ë¶„
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.gpt_messages = []
        st.session_state.gemini_messages = []
        st.rerun()

    st.divider()

    uploaded_files = st.file_uploader(
        "PDF ì—…ë¡œë“œ",
        type="pdf",
        accept_multiple_files=True
    )

    if uploaded_files and st.button("í•™ìŠµ ì‹œì‘"):
        # --- ì—¬ê¸°ì„œë¶€í„° ìˆ˜ì • (ì§„í–‰ ë°” ë° í…ìŠ¤íŠ¸ ê³µê°„ í™•ë³´) ---
        status_placeholder = st.empty()
        progress_bar = st.progress(0)
        
        with st.spinner("ìë£Œ ë¶„ì„ ì¤‘..."):
            all_docs = []

            for i, file in enumerate(uploaded_files):
                with tempfile.NamedTemporaryFile(delete=False) as tmp:
                    tmp.write(file.getvalue())
                    tmp_path = tmp.name

                # ğŸ§ í˜„ì¬ ì–´ë–¤ íŒŒì¼ì„ ë¶„ì„ ì¤‘ì¸ì§€ í‘œì‹œ
                status_placeholder.info(f"ğŸ“„ '{file.name}' ë¶„ì„ ì¤‘... (íŒŒì¼ {i+1}/{len(uploaded_files)})")
                
                # í•˜ì´ë¸Œë¦¬ë“œ OCR í•¨ìˆ˜ í˜¸ì¶œ
                docs = extract_documents_from_pdf(
                    tmp_path,
                    source_name=file.name
                )
                all_docs.extend(docs)
                os.remove(tmp_path)
                
                # íŒŒì¼ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress(int((i + 1) / len(uploaded_files) * 50))

            status_placeholder.info("ğŸ§  ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘... (ê±°ì˜ ë‹¤ ëì–´ìš”!)")
            
            chunks = split_documents(all_docs)
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001"
            )

            st.session_state.vector_db = FAISS.from_documents(
                chunks, embedding=embeddings
            )  

            # ëª¨ë“  ê³¼ì • ì™„ë£Œ ì²˜ë¦¬
            progress_bar.progress(100)
            status_placeholder.success(f"âœ… ì´ {len(uploaded_files)}ê°œì˜ íŒŒì¼ í•™ìŠµ ì™„ë£Œ!")
            # --- ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • ---

# --------------------------------
# --------------------------------
# ì±„íŒ… UI (ì´ì „ ëŒ€í™” ê¸°ë¡ ë³µì›)
# --------------------------------
# í™”ë©´ì„ 2ê°œë¡œ ë‚˜ëˆ ì„œ ê° ëª¨ë¸ì˜ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¢Œìš°ì— ë°°ì¹˜í•©ë‹ˆë‹¤.
view_col1, view_col2 = st.columns(2)

with view_col1:
    for msg in st.session_state.gpt_messages:
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ GPTì˜ ë‹µë³€ì„ ì°¨ë¡€ë¡œ ì¶œë ¥
        avatar = "ğŸ¤–" if msg["role"] == "assistant" else None
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

with view_col2:
    for msg in st.session_state.gemini_messages:
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ Geminiì˜ ë‹µë³€ì„ ì°¨ë¡€ë¡œ ì¶œë ¥
        avatar = "â™Š" if msg["role"] == "assistant" else None
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

# --------------------------------
# ì‹ ê·œ ì§ˆë¬¸ ì…ë ¥ ë° ì²˜ë¦¬ (ë³‘ë ¬ & ê³µí†µ ê²€ìƒ‰ ë²„ì „)
# --------------------------------

if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    if st.session_state.vector_db is None:
        st.warning("ë¨¼ì € PDFë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
    else:
        # 1. ê³µí†µ ê²€ìƒ‰
        with st.spinner("ìë£Œ ì°¾ëŠ” ì¤‘..."):
            retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": 5})
            shared_docs = retriever.invoke(question)

        # 2. ë©”ì‹œì§€ ê¸°ë¡
        st.session_state.gpt_messages.append({"role": "user", "content": question})
        st.session_state.gemini_messages.append({"role": "user", "content": question})
        
        with st.chat_message("user"):
            st.markdown(question)

        # 3. í™”ë©´ ê³µê°„ í™•ë³´
        col1, col2 = st.columns(2)
        with col1:
            st.info("ğŸ¤– GPT-5.2")
            area_gpt = st.empty()
        with col2:
            st.info("â™Š Gemini 2.5")
            area_gem = st.empty()

        # 4. ìƒì„±ê¸° ìƒì„±
        gen_gpt = run_rag_stream(question, answer_style, "gpt", st.session_state.gpt_messages[:-1], shared_docs)
        gen_gem = run_rag_stream(question, answer_style, "gemini", st.session_state.gemini_messages[:-1], shared_docs)

        # 5. ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (Queue ë°©ì‹)
        q_gpt, q_gem = queue.Queue(), queue.Queue()
        
        def produce(gen, q):
            for chunk in gen:
                q.put(chunk)
            q.put(None)

        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            executor.submit(produce, gen_gpt, q_gpt)
            executor.submit(produce, gen_gem, q_gem)

            full_gpt, full_gem = "", ""
            gpt_done, gem_done = False, False

            while not (gpt_done and gem_done):
                # GPT ë°ì´í„° ì—…ë°ì´íŠ¸
                while not q_gpt.empty():
                    item = q_gpt.get()
                    if item is None: gpt_done = True
                    else: 
                        full_gpt += item
                        area_gpt.markdown(full_gpt + "â–Œ")
                
                # Gemini ë°ì´í„° ì—…ë°ì´íŠ¸
                while not q_gem.empty():
                    item = q_gem.get()
                    if item is None: gem_done = True
                    else: 
                        full_gem += item
                        area_gem.markdown(full_gem + "â–Œ")
                
                time.sleep(0.05)

        # 6. ë§ˆë¬´ë¦¬ ì €ì¥
        area_gpt.markdown(full_gpt)
        area_gem.markdown(full_gem)
        st.session_state.gpt_messages.append({"role": "assistant", "content": full_gpt})
        st.session_state.gemini_messages.append({"role": "assistant", "content": full_gem})