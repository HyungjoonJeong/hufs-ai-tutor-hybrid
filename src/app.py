import streamlit as st
import os
import tempfile
import queue
import time
import concurrent.futures # íŒŒì¼ ìµœìƒë‹¨ì— ì¶”ê°€í•´ì£¼ì„¸ìš”!
from dotenv import load_dotenv

# 1. í•µì‹¬ ì„¤ê³„ë„ (Core)
from langchain_core.prompts import PromptTemplate

# 2. êµ¬ê¸€ AI ì—°ê²° (Google GenAI)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


# 3. ë°ì´í„°ë² ì´ìŠ¤ (Community)
from langchain_community.vectorstores import FAISS

# 4. ë©”ëª¨ë¦¬ (ê°€ì¥ ì•ˆì „í•œ ìµœì‹  ê²½ë¡œë¡œ ë³€ê²½)
# ë§Œì•½ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ from langchain_community.chat_message_histories import ... ë¡œ ì„ íšŒí•´ì•¼ í•©ë‹ˆë‹¤.
#try:
#    from langchain.memory import ConversationBufferMemory
#except ImportError:
#    from langchain_community.memory import ConversationBufferMemory

# 5. ê²€ìƒ‰ ì—”ì§„ (Classic)
from langchain_classic.chains import RetrievalQA

# 6. ì§ì ‘ ë§Œë“  ëª¨ë“ˆ
from extract_text import extract_documents_from_pdf, split_documents

# --------------------------------
# ê¸°ë³¸ ì„¤ì •
# --------------------------------
load_dotenv()

st.set_page_config(
    page_title="HUFS AI Tutor",
    layout="wide"
)

st.title("HUFS RAG ê¸°ë°˜ AI íŠœí„° (GPT-5.2 & Gemini 2.5)")
st.caption("ê°•ì˜ ìë£Œ ê¸°ë°˜ìœ¼ë¡œ GPTì™€ Geminië¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ë©° ì¶œì²˜ë¥¼ ëª…í™•íˆ ì œì‹œí•©ë‹ˆë‹¤.")

# --------------------------------
# ì„¸ì…˜ ìƒíƒœ
# --------------------------------
# --- app.py ìƒë‹¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë¶€ë¶„ ---

if "gpt_messages" not in st.session_state:
    st.session_state.gpt_messages = []

if "gemini_messages" not in st.session_state:
    st.session_state.gemini_messages = []

# ê¸°ì¡´ messagesëŠ” ë” ì´ìƒ ì“°ì§€ ì•Šì§€ë§Œ, 
# í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‚¨ê²¨ë‘ê±°ë‚˜ ì•„ë˜ì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
# if "messages" not in st.session_state:
#     st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# --------------------------------
# ì§ˆë¬¸ ë¶„ë¥˜ê¸°
# --------------------------------
def classify_question(question: str) -> str:
    # í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” ì„¤ì •ì´ ë³µì¡í•œ Gemini ëŒ€ì‹  GPT-4o-minië¥¼ ì”ë‹ˆë‹¤. (ë§¤ìš° ì €ë ´)
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì„ 'concept', 'calculation', 'summary' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´. í•œ ë‹¨ì–´ë§Œ ë‹µí•´. ì§ˆë¬¸: {question}"
    result = llm.invoke(prompt)
    return result.content.strip().lower()


# --------------------------------
# --------------------------------
# ê³„ì‚° ë¬¸ì œ ì „ìš© ì²´ì¸ (GPT/Gemini ëŒ€ì‘)
# --------------------------------
def run_calculation_chain(question: str, model_type: str, vector_db):
    # 1. ëª¨ë¸ ì„ íƒ
    if model_type == "gpt":
        llm = ChatOpenAI(model="gpt-5.2", temperature=0)
    else:
        # 2026ë…„ ê¸°ì¤€ ìµœì‹  ì•ˆì • ë²„ì „ì¸ 1.5-flash ê¶Œì¥
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

    # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
# í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ st.session_state.vector_db ëŒ€ì‹  vector_db ì‚¬ìš©!
    docs = vector_db.similarity_search(question, k=7)
    context = "\n\n".join([d.page_content for d in docs])

    template = """
ë„ˆëŠ” ëŒ€í•™ ê³¼ëª© ê³„ì‚° ë¬¸ì œë¥¼ í‘¸ëŠ” ì¡°êµì´ë‹¤. ì œê³µëœ [ë¬¸ë§¥]ì˜ ê³µì‹ê³¼ ìˆ˜ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ì–´ë¼.

[ê·œì¹™]
1. í’€ì´ ê³¼ì •ì„ ê°€ë…ì„±ì„ ìœ„í•´ ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ë°˜ë“œì‹œ ìˆ«ì ì¸ë±ìŠ¤(1., 2., 3.)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ì—¬ ìƒì„¸íˆ ì„¤ëª…í•˜ë¼.
2. ìˆ˜ì‹ì€ LaTeX í˜•ì‹ì´ë‚˜ ëª…í™•í•œ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œì‹œí•˜ë¼.
3. ë§ˆì§€ë§‰ì— ìµœì¢… ë‹µì„ 'ì •ë‹µ: 'ê³¼ í•¨ê»˜ ì •ë¦¬í•˜ë¼.
4. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ê°€ê¸‰ì  ì‚¬ìš©í•˜ì§€ ë§ê³ , ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ë¼ê³  ì•ˆë‚´í•˜ë¼.
2. ê° í¬ì¸íŠ¸ë§ˆë‹¤ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë³¼ë“œì²´(**)ë¡œ í‘œì‹œí•˜ë¼.
3. ê°•ì˜ ìë£Œì˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë˜, ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ë¼.
5. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
6. ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œì™€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ë¼.
7. ë‹µë³€ì€ ìµœì†Œ 3ë¬¸ë‹¨ ì´ìƒì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.
8. ê°•ì˜ ìë£Œì— ìˆëŠ” ì˜ˆì‹œë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•  ê²ƒ.
9. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµì„ ë•ê¸° ìœ„í•´ 'ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê°œë…'ì„ ë‘ì„¸ ë¬¸ì¥ ë§ë¶™ì¼ ê²ƒ.

[ë¬¸ë§¥]
{context}

[ë¬¸ì œ]
{question}

[í’€ì´]
"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=template
    )

    # 3. ë‹µë³€ ìƒì„±
    response = llm.invoke(
        prompt.format(
            context=context,
            question=question
        )
    )

    return response.content, docs
# --------------------------------
# ì¼ë°˜ RAG ì²´ì¸
# --------------------------------
# 2. ë©”ì¸ ë‹µë³€ ì²´ì¸ (GPT-4o ì‚¬ìš© - ì •ë°€í•œ ë…¼ë¦¬)
# run_rag ì •ì˜ ë¶€ë¶„ ìˆ˜ì •
def run_rag_stream(question: str, answer_style: str, model_type: str, chat_history: list, docs: list):
    # 1. ëª¨ë¸ ì„¤ì • (ì•ˆì •ì ì¸ ëª¨ë¸ëª… ì‚¬ìš©)
    if model_type == "gpt":
        # streaming=Trueë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        llm = ChatOpenAI(model="gpt-4o", temperature=0.7, streaming=True)
    else:
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.7)

    # 2. ì»¨í…ìŠ¤íŠ¸ ë° íˆìŠ¤í† ë¦¬ êµ¬ì„±
    context_text = "\n\n".join([d.page_content for d in docs])
    chat_history_str = "\n".join([f"{m['role']}: {m['content']}" for m in (chat_history or [])])
    
    length_instruction = (
        "í•µì‹¬ ìœ„ì£¼ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ê°„ê²°í•˜ê²Œ ë‹µí•˜ë¼." if answer_style == "ì§§ê²Œ" 
        else "ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ìì„¸íˆ ë‹µí•˜ë¼."
    )

    template = """
ë‹¹ì‹ ì€ í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµì˜ 1íƒ€ ê°•ì‚¬ AI íŠœí„°ì…ë‹ˆë‹¤. 
ì œê³µëœ [ê°•ì˜ ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ê·œì¹™]
1. ê°€ë…ì„±ì„ ìœ„í•´ ë°˜ë“œì‹œ ìˆ«ì ì¸ë±ìŠ¤(1., 2., 3.)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ë¼.
2. ê° í¬ì¸íŠ¸ë§ˆë‹¤ í•µì‹¬ ì œëª©ì„ ë³¼ë“œì²´(**)ë¡œ í‘œì‹œí•˜ë¼.
3. ê°•ì˜ ìë£Œì˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë˜, ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ë¼.
4. ë°˜ë“œì‹œ ë¬¸ë§¥ì— ê·¼ê±°í•´ ë‹µí•˜ë¼.
5. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
6. ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œì™€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ë¼.
7. ë‹µë³€ì€ ìµœì†Œ 3ë¬¸ë‹¨ ì´ìƒì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.
8. ê°•ì˜ ìë£Œì— ìˆëŠ” ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•  ê²ƒ.
9. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµì„ ë•ê¸° ìœ„í•´ 'ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê°œë…'ì„ ë‘ì„¸ ë¬¸ì¥ ë§ë¶™ì¼ ê²ƒ.
10. {length_instruction}

[ì´ì „ ëŒ€í™”]
{chat_history}

[ë¬¸ë§¥]
{context}

[ì§ˆë¬¸]
{question}

ë‹µë³€ (ì „ë¬¸ì ì´ê³  ìƒì„¸í•˜ê²Œ):
"""
    prompt = template.format(
        length_instruction=length_instruction,
        chat_history=chat_history_str,
        context=context_text,
        question=question
    )

    # 3. í•µì‹¬: return ëŒ€ì‹  yield ì‚¬ìš©
    # llm.invoke ëŒ€ì‹  llm.streamì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    for chunk in llm.stream(prompt):
        # ëª¨ë¸ë³„ë¡œ chunkì—ì„œ ë‚´ìš©ì„ êº¼ë‚´ëŠ” ë°©ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        if hasattr(chunk, 'content'):
            content = chunk.content
        else:
            content = str(chunk)
            
        if content:
            yield content  # í•œ ê¸€ì(ë˜ëŠ” í•œ ë‹¨ì–´)ì”© ë°–ìœ¼ë¡œ ë‚´ë³´ëƒ„


    response = llm.invoke(prompt)
    return response.content



# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
with st.sidebar:
    st.header("ì„¤ì •")

    answer_style = st.radio(
        "ë‹µë³€ ê¸¸ì´",
        ["ì§§ê²Œ", "ìì„¸íˆ"],
        index=1
    )

    # ì‚¬ì´ë“œë°”ì˜ ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ë¶€ë¶„
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.gpt_messages = []
        st.session_state.gemini_messages = []
        st.rerun()

    st.divider()

    uploaded_files = st.file_uploader(
        "PDF ì—…ë¡œë“œ",
        type="pdf",
        accept_multiple_files=True
    )

    if uploaded_files and st.button("í•™ìŠµ ì‹œì‘"):
        # --- ì—¬ê¸°ì„œë¶€í„° ìˆ˜ì • (ì§„í–‰ ë°” ë° í…ìŠ¤íŠ¸ ê³µê°„ í™•ë³´) ---
        status_placeholder = st.empty()
        progress_bar = st.progress(0)
        
        with st.spinner("ìë£Œ ë¶„ì„ ì¤‘..."):
            all_docs = []

            for i, file in enumerate(uploaded_files):
                with tempfile.NamedTemporaryFile(delete=False) as tmp:
                    tmp.write(file.getvalue())
                    tmp_path = tmp.name

                # ğŸ§ í˜„ì¬ ì–´ë–¤ íŒŒì¼ì„ ë¶„ì„ ì¤‘ì¸ì§€ í‘œì‹œ
                status_placeholder.info(f"ğŸ“„ '{file.name}' ë¶„ì„ ì¤‘... (íŒŒì¼ {i+1}/{len(uploaded_files)})")
                
                # í•˜ì´ë¸Œë¦¬ë“œ OCR í•¨ìˆ˜ í˜¸ì¶œ
                docs = extract_documents_from_pdf(
                    tmp_path,
                    source_name=file.name
                )
                all_docs.extend(docs)
                os.remove(tmp_path)
                
                # íŒŒì¼ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress(int((i + 1) / len(uploaded_files) * 50))

            status_placeholder.info("ğŸ§  ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘... (ê±°ì˜ ë‹¤ ëì–´ìš”!)")
            
            chunks = split_documents(all_docs)
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001"
            )

            st.session_state.vector_db = FAISS.from_documents(
                chunks, embedding=embeddings
            )  

            # ëª¨ë“  ê³¼ì • ì™„ë£Œ ì²˜ë¦¬
            progress_bar.progress(100)
            status_placeholder.success(f"âœ… ì´ {len(uploaded_files)}ê°œì˜ íŒŒì¼ í•™ìŠµ ì™„ë£Œ!")
            # --- ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • ---

# --------------------------------
# --------------------------------
# ì±„íŒ… UI (ì´ì „ ëŒ€í™” ê¸°ë¡ ë³µì›)
# --------------------------------
# í™”ë©´ì„ 2ê°œë¡œ ë‚˜ëˆ ì„œ ê° ëª¨ë¸ì˜ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¢Œìš°ì— ë°°ì¹˜í•©ë‹ˆë‹¤.
view_col1, view_col2 = st.columns(2)

with view_col1:
    for msg in st.session_state.gpt_messages:
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ GPTì˜ ë‹µë³€ì„ ì°¨ë¡€ë¡œ ì¶œë ¥
        avatar = "ğŸ¤–" if msg["role"] == "assistant" else None
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

with view_col2:
    for msg in st.session_state.gemini_messages:
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ Geminiì˜ ë‹µë³€ì„ ì°¨ë¡€ë¡œ ì¶œë ¥
        avatar = "â™Š" if msg["role"] == "assistant" else None
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

# --------------------------------
# ì‹ ê·œ ì§ˆë¬¸ ì…ë ¥ ë° ì²˜ë¦¬ (ë³‘ë ¬ & ê³µí†µ ê²€ìƒ‰ ë²„ì „)
# --------------------------------

if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    if st.session_state.vector_db is None:
        st.warning("ë¨¼ì € PDFë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
    else:
        # 1. ê³µí†µ ê²€ìƒ‰ (ë©”ì¸ ìŠ¤ë ˆë“œ)
        with st.spinner("ìë£Œ ì°¾ëŠ” ì¤‘..."):
            retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": 7})
            shared_docs = retriever.invoke(question)

        # 2. ë©”ì‹œì§€ ê¸°ë¡ ì €ì¥
        st.session_state.gpt_messages.append({"role": "user", "content": question})
        st.session_state.gemini_messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        # 3. ë ˆì´ì•„ì›ƒ ë° ë¹ˆ ê³µê°„ ìƒì„±
        col1, col2 = st.columns(2)
        with col1:
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.subheader("GPT-5.2")
                area_gpt = st.empty()  # GPTê°€ ì¨ì§ˆ ê³µê°„
        with col2:
            with st.chat_message("assistant", avatar="â™Š"):
                st.subheader("Gemini 2.5")
                area_gem = st.empty()  # Geminiê°€ ì¨ì§ˆ ê³µê°„

        # 4. ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (í•µì‹¬ ë¡œì§)
        gen_gpt = run_rag_stream(question, answer_style, "gpt", st.session_state.gpt_messages[:-1], shared_docs)
        gen_gem = run_rag_stream(question, answer_style, "gemini", st.session_state.gemini_messages[:-1], shared_docs)

        full_gpt, full_gem = "", ""
        
        # ë‘ ìƒì„±ê¸°(Generator)ë¥¼ ë³‘ë ¬ë¡œ ëŒë¦¬ë©° í™”ë©´ ì—…ë°ì´íŠ¸
        with concurrent.futures.ThreadPoolExecutor() as executor:
            # ê°ê°ì˜ ìŠ¤íŠ¸ë¦¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ í•œ ë²ˆì— ì²˜ë¦¬í•˜ê¸° ìœ„í•´ zip_longestì™€ ìœ ì‚¬í•œ ë¡œì§ ì‚¬ìš©
            # ì—¬ê¸°ì„œëŠ” ë£¨í”„ë¥¼ ëŒë©° í•˜ë‚˜ì”© ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            
            # ì£¼ì˜: Streamlitì€ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œë§Œ UI ì—…ë°ì´íŠ¸ë¥¼ ê¶Œì¥í•˜ë¯€ë¡œ,
            # ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê±´ ë³‘ë ¬ë¡œ í•˜ë˜ ë¿Œë¦¬ëŠ” ê±´ ë£¨í”„ë¥¼ í™œìš©í•©ë‹ˆë‹¤.
            import queue
            q_gpt, q_gem = queue.Queue(), queue.Queue()

            def produce(gen, q):
                for chunk in gen:
                    q.put(chunk)
                q.put(None) # ë ì‹ í˜¸

            executor.submit(produce, gen_gpt, q_gpt)
            executor.submit(produce, gen_gem, q_gem)

            gpt_done, gem_done = False, False
            while not (gpt_done and gem_done):
                # GPT í•œ ê¸€ì ê°€ì ¸ì™€ì„œ ì—…ë°ì´íŠ¸
                try:
                    chunk = q_gpt.get_nowait()
                    if chunk is None: gpt_done = True
                    else:
                        full_gpt += chunk
                        area_gpt.markdown(full_gpt + "â–Œ") # ì»¤ì„œ íš¨ê³¼
                except queue.Empty:
                    pass

                # Gemini í•œ ê¸€ì ê°€ì ¸ì™€ì„œ ì—…ë°ì´íŠ¸
                try:
                    chunk = q_gem.get_nowait()
                    if chunk is None: gem_done = True
                    else:
                        full_gem += chunk
                        area_gem.markdown(full_gem + "â–Œ") # ì»¤ì„œ íš¨ê³¼
                except queue.Empty:
                    pass
                
                import time
                time.sleep(0.01) # UI ë Œë”ë§ì„ ìœ„í•œ ì•„ì£¼ ì§§ì€ íœ´ì‹

        # 5. ìµœì¢… ë‹µë³€ ì •ë¦¬ (ì»¤ì„œ ì œê±° ë° ì°¸ê³ ë¬¸í—Œ ì¶”ê°€)
        refs = set([f"- {d.metadata['source']} p.{d.metadata['page'] + 1}" for d in shared_docs])
        ref_text = "\n\n---\n**ì°¸ê³ :**\n" + "\n".join(sorted(refs))
        
        area_gpt.markdown(full_gpt + ref_text)
        area_gem.markdown(full_gem + ref_text)
        
        st.session_state.gpt_messages.append({"role": "assistant", "content": full_gpt + ref_text})
        st.session_state.gemini_messages.append({"role": "assistant", "content": full_gem + ref_text})