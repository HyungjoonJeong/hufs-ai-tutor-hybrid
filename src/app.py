import streamlit as st
import os
import tempfile
import concurrent.futures # íŒŒì¼ ìµœìƒë‹¨ì— ì¶”ê°€í•´ì£¼ì„¸ìš”!
from dotenv import load_dotenv

# 1. í•µì‹¬ ì„¤ê³„ë„ (Core)
from langchain_core.prompts import PromptTemplate

# 2. êµ¬ê¸€ AI ì—°ê²° (Google GenAI)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


# 3. ë°ì´í„°ë² ì´ìŠ¤ (Community)
from langchain_community.vectorstores import FAISS

# 4. ë©”ëª¨ë¦¬ (ê°€ì¥ ì•ˆì „í•œ ìµœì‹  ê²½ë¡œë¡œ ë³€ê²½)
# ë§Œì•½ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ from langchain_community.chat_message_histories import ... ë¡œ ì„ íšŒí•´ì•¼ í•©ë‹ˆë‹¤.
#try:
#    from langchain.memory import ConversationBufferMemory
#except ImportError:
#    from langchain_community.memory import ConversationBufferMemory

# 5. ê²€ìƒ‰ ì—”ì§„ (Classic)
from langchain_classic.chains import RetrievalQA

# 6. ì§ì ‘ ë§Œë“  ëª¨ë“ˆ
from extract_text import extract_documents_from_pdf, split_documents

# --------------------------------
# ê¸°ë³¸ ì„¤ì •
# --------------------------------
load_dotenv()

st.set_page_config(
    page_title="HUFS AI Tutor",
    layout="wide"
)

st.title("HUFS RAG ê¸°ë°˜ AI íŠœí„° (GPT-5.2 & Gemini 2.5)")
st.caption("ê°•ì˜ ìë£Œ ê¸°ë°˜ìœ¼ë¡œ GPTì™€ Geminië¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ë©° ì¶œì²˜ë¥¼ ëª…í™•íˆ ì œì‹œí•©ë‹ˆë‹¤.")

# --------------------------------
# ì„¸ì…˜ ìƒíƒœ
# --------------------------------
# --- app.py ìƒë‹¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë¶€ë¶„ ---

if "gpt_messages" not in st.session_state:
    st.session_state.gpt_messages = []

if "gemini_messages" not in st.session_state:
    st.session_state.gemini_messages = []

# ê¸°ì¡´ messagesëŠ” ë” ì´ìƒ ì“°ì§€ ì•Šì§€ë§Œ, 
# í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‚¨ê²¨ë‘ê±°ë‚˜ ì•„ë˜ì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
# if "messages" not in st.session_state:
#     st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# --------------------------------
# ì§ˆë¬¸ ë¶„ë¥˜ê¸°
# --------------------------------
def classify_question(question: str) -> str:
    # í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” ì„¤ì •ì´ ë³µì¡í•œ Gemini ëŒ€ì‹  GPT-4o-minië¥¼ ì”ë‹ˆë‹¤. (ë§¤ìš° ì €ë ´)
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì„ 'concept', 'calculation', 'summary' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´. í•œ ë‹¨ì–´ë§Œ ë‹µí•´. ì§ˆë¬¸: {question}"
    result = llm.invoke(prompt)
    return result.content.strip().lower()


# --------------------------------
# --------------------------------
# ê³„ì‚° ë¬¸ì œ ì „ìš© ì²´ì¸ (GPT/Gemini ëŒ€ì‘)
# --------------------------------
def run_calculation_chain(question: str, model_type: str, vector_db):
    # 1. ëª¨ë¸ ì„ íƒ
    if model_type == "gpt":
        llm = ChatOpenAI(model="gpt-5.2", temperature=0)
    else:
        # 2026ë…„ ê¸°ì¤€ ìµœì‹  ì•ˆì • ë²„ì „ì¸ 1.5-flash ê¶Œì¥
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

    # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
# í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ st.session_state.vector_db ëŒ€ì‹  vector_db ì‚¬ìš©!
    docs = vector_db.similarity_search(question, k=7)
    context = "\n\n".join([d.page_content for d in docs])

    template = """
ë„ˆëŠ” ëŒ€í•™ ê³¼ëª© ê³„ì‚° ë¬¸ì œë¥¼ í‘¸ëŠ” ì¡°êµì´ë‹¤. ì œê³µëœ [ë¬¸ë§¥]ì˜ ê³µì‹ê³¼ ìˆ˜ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ì–´ë¼.

[ê·œì¹™]
1. í’€ì´ ê³¼ì •ì„ ê°€ë…ì„±ì„ ìœ„í•´ ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ë°˜ë“œì‹œ ìˆ«ì ì¸ë±ìŠ¤(1., 2., 3.)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ì—¬ ìƒì„¸íˆ ì„¤ëª…í•˜ë¼.
2. ìˆ˜ì‹ì€ LaTeX í˜•ì‹ì´ë‚˜ ëª…í™•í•œ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œì‹œí•˜ë¼.
3. ë§ˆì§€ë§‰ì— ìµœì¢… ë‹µì„ 'ì •ë‹µ: 'ê³¼ í•¨ê»˜ ì •ë¦¬í•˜ë¼.
4. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ê°€ê¸‰ì  ì‚¬ìš©í•˜ì§€ ë§ê³ , ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ë¼ê³  ì•ˆë‚´í•˜ë¼.
2. ê° í¬ì¸íŠ¸ë§ˆë‹¤ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë³¼ë“œì²´(**)ë¡œ í‘œì‹œí•˜ë¼.
3. ê°•ì˜ ìë£Œì˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë˜, ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ë¼.
5. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
6. ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œì™€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ë¼.
7. ë‹µë³€ì€ ìµœì†Œ 3ë¬¸ë‹¨ ì´ìƒì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.
8. ê°•ì˜ ìë£Œì— ìˆëŠ” ì˜ˆì‹œë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•  ê²ƒ.
9. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµì„ ë•ê¸° ìœ„í•´ 'ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê°œë…'ì„ ë‘ì„¸ ë¬¸ì¥ ë§ë¶™ì¼ ê²ƒ.

[ë¬¸ë§¥]
{context}

[ë¬¸ì œ]
{question}

[í’€ì´]
"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=template
    )

    # 3. ë‹µë³€ ìƒì„±
    response = llm.invoke(
        prompt.format(
            context=context,
            question=question
        )
    )

    return response.content, docs
# --------------------------------
# ì¼ë°˜ RAG ì²´ì¸
# --------------------------------
# 2. ë©”ì¸ ë‹µë³€ ì²´ì¸ (GPT-4o ì‚¬ìš© - ì •ë°€í•œ ë…¼ë¦¬)
# run_rag ì •ì˜ ë¶€ë¶„ ìˆ˜ì •
def run_rag_final(question: str, answer_style: str, model_type: str, chat_history: list, docs: list):
    # 1. ëª¨ë¸ ì„¤ì • (ì•ˆì •ì ì¸ ëª¨ë¸ëª…ìœ¼ë¡œ ìˆ˜ì •)
    if model_type == "gpt":
        llm = ChatOpenAI(model="gpt-5.2", temperature=0.7)
    else:
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)

    # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = "\n\n".join([d.page_content for d in docs])
    chat_history_str = "\n".join([f"{m['role']}: {m['content']}" for m in chat_history])
    
    length_instruction = (
        "í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ë¼." if answer_style == "ì§§ê²Œ" 
        else "ìì„¸íˆ ì„¤ëª…í•˜ë¼."
    )

    template = """
ë‹¹ì‹ ì€ í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµì˜ 1íƒ€ ê°•ì‚¬ AI íŠœí„°ì…ë‹ˆë‹¤. 
ì œê³µëœ [ê°•ì˜ ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ê·œì¹™]
1. ê°€ë…ì„±ì„ ìœ„í•´ ë°˜ë“œì‹œ ìˆ«ì ì¸ë±ìŠ¤(1., 2., 3.)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ë¼.
2. ê° í¬ì¸íŠ¸ë§ˆë‹¤ í•µì‹¬ ì œëª©ì„ ë³¼ë“œì²´(**)ë¡œ í‘œì‹œí•˜ë¼.
3. ê°•ì˜ ìë£Œì˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ë˜, ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ë¼.
4. ë°˜ë“œì‹œ ë¬¸ë§¥ì— ê·¼ê±°í•´ ë‹µí•˜ë¼.
5. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
6. ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œì™€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ë¼.
7. ë‹µë³€ì€ ìµœì†Œ 3ë¬¸ë‹¨ ì´ìƒì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.
8. ê°•ì˜ ìë£Œì— ìˆëŠ” ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•  ê²ƒ.
9. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµì„ ë•ê¸° ìœ„í•´ 'ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê°œë…'ì„ ë‘ì„¸ ë¬¸ì¥ ë§ë¶™ì¼ ê²ƒ.
10. {length_instruction}

[ì´ì „ ëŒ€í™”]
{chat_history}

[ë¬¸ë§¥]
{context}

[ì§ˆë¬¸]
{question}

ë‹µë³€ (ì „ë¬¸ì ì´ê³  ìƒì„¸í•˜ê²Œ):
"""
    prompt = template.format(
        length_instruction=length_instruction,
        chat_history=chat_history_str,
        context=context_text,
        question=question
    )

    response = llm.invoke(prompt)
    return response.content



# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
with st.sidebar:
    st.header("ì„¤ì •")

    answer_style = st.radio(
        "ë‹µë³€ ê¸¸ì´",
        ["ì§§ê²Œ", "ìì„¸íˆ"],
        index=1
    )

    # ì‚¬ì´ë“œë°”ì˜ ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ë¶€ë¶„
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.gpt_messages = []
        st.session_state.gemini_messages = []
        st.rerun()

    st.divider()

    uploaded_files = st.file_uploader(
        "PDF ì—…ë¡œë“œ",
        type="pdf",
        accept_multiple_files=True
    )

    if uploaded_files and st.button("í•™ìŠµ ì‹œì‘"):
        # --- ì—¬ê¸°ì„œë¶€í„° ìˆ˜ì • (ì§„í–‰ ë°” ë° í…ìŠ¤íŠ¸ ê³µê°„ í™•ë³´) ---
        status_placeholder = st.empty()
        progress_bar = st.progress(0)
        
        with st.spinner("ìë£Œ ë¶„ì„ ì¤‘..."):
            all_docs = []

            for i, file in enumerate(uploaded_files):
                with tempfile.NamedTemporaryFile(delete=False) as tmp:
                    tmp.write(file.getvalue())
                    tmp_path = tmp.name

                # ğŸ§ í˜„ì¬ ì–´ë–¤ íŒŒì¼ì„ ë¶„ì„ ì¤‘ì¸ì§€ í‘œì‹œ
                status_placeholder.info(f"ğŸ“„ '{file.name}' ë¶„ì„ ì¤‘... (íŒŒì¼ {i+1}/{len(uploaded_files)})")
                
                # í•˜ì´ë¸Œë¦¬ë“œ OCR í•¨ìˆ˜ í˜¸ì¶œ
                docs = extract_documents_from_pdf(
                    tmp_path,
                    source_name=file.name
                )
                all_docs.extend(docs)
                os.remove(tmp_path)
                
                # íŒŒì¼ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress(int((i + 1) / len(uploaded_files) * 50))

            status_placeholder.info("ğŸ§  ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘... (ê±°ì˜ ë‹¤ ëì–´ìš”!)")
            
            chunks = split_documents(all_docs)
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001"
            )

            st.session_state.vector_db = FAISS.from_documents(
                chunks, embedding=embeddings
            )  

            # ëª¨ë“  ê³¼ì • ì™„ë£Œ ì²˜ë¦¬
            progress_bar.progress(100)
            status_placeholder.success(f"âœ… ì´ {len(uploaded_files)}ê°œì˜ íŒŒì¼ í•™ìŠµ ì™„ë£Œ!")
            # --- ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • ---

# --------------------------------
# --------------------------------
# ì±„íŒ… UI (ì´ì „ ëŒ€í™” ê¸°ë¡ ë³µì›)
# --------------------------------
# í™”ë©´ì„ 2ê°œë¡œ ë‚˜ëˆ ì„œ ê° ëª¨ë¸ì˜ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¢Œìš°ì— ë°°ì¹˜í•©ë‹ˆë‹¤.
view_col1, view_col2 = st.columns(2)

with view_col1:
    for msg in st.session_state.gpt_messages:
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ GPTì˜ ë‹µë³€ì„ ì°¨ë¡€ë¡œ ì¶œë ¥
        avatar = "ğŸ¤–" if msg["role"] == "assistant" else None
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

with view_col2:
    for msg in st.session_state.gemini_messages:
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ Geminiì˜ ë‹µë³€ì„ ì°¨ë¡€ë¡œ ì¶œë ¥
        avatar = "â™Š" if msg["role"] == "assistant" else None
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

# --------------------------------
# ì‹ ê·œ ì§ˆë¬¸ ì…ë ¥ ë° ì²˜ë¦¬ (ë³‘ë ¬ & ê³µí†µ ê²€ìƒ‰ ë²„ì „)
# --------------------------------

if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    if st.session_state.vector_db is None:
        st.warning("ë¨¼ì € PDFë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
    else:
        # 1. ê³µí†µ ì¬ë£Œ ì¤€ë¹„ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ í•œ ë²ˆë§Œ ê²€ìƒ‰!)
        # ì´ë ‡ê²Œ í•˜ë©´ ìŠ¤ë ˆë“œ ë‚´ë¶€ AttributeErrorì™€ êµ¬ê¸€ API ì¶©ëŒì„ ì™„ë²½íˆ ë§‰ìŠµë‹ˆë‹¤.
        with st.spinner("ê´€ë ¨ ê°•ì˜ ìë£Œë¥¼ ì°¾ëŠ” ì¤‘..."):
            retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": 7})
            shared_docs = retriever.invoke(question) # ê³µí†µ ê²€ìƒ‰ ê²°ê³¼

        gpt_h = st.session_state.gpt_messages.copy()
        gem_h = st.session_state.gemini_messages.copy()
        
        # ì„¸ì…˜ì— ì‚¬ìš©ì ì§ˆë¬¸ ì¦‰ì‹œ ì €ì¥
        st.session_state.gpt_messages.append({"role": "user", "content": question})
        st.session_state.gemini_messages.append({"role": "user", "content": question})

        # í™”ë©´ì— ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(question)

        # 2. ë ˆì´ì•„ì›ƒ ì„¤ì • ë° ì§ˆë¬¸ ë¶„ë¥˜
        col1, col2 = st.columns(2)
        q_type = classify_question(question)

        # 3. ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ ì •ì˜
        # íŒ: ì´ì œ run_ragëŠ” vector_db ëŒ€ì‹  ê²€ìƒ‰ëœ docsë¥¼ ì§ì ‘ ë°›ë„ë¡ ì•„ë˜ì—ì„œ ìˆ˜ì •í•  ê²ë‹ˆë‹¤.
        def fetch_gpt():
            try:
                if q_type == "calculation":
                    ans, _ = run_calculation_chain(question, "gpt", st.session_state.vector_db)
                else:
                    # run_ragì— shared_docsë¥¼ ì§ì ‘ ë„˜ê²¨ì¤ë‹ˆë‹¤.
                    ans = run_rag_final(question, answer_style, "gpt", gpt_h, shared_docs)
                
                refs = set([f"- {d.metadata['source']} p.{d.metadata['page'] + 1}" for d in shared_docs])
                return f"{ans}\n\n---\n**ì°¸ê³ :**\n" + "\n".join(sorted(refs))
            except Exception as e:
                return f"âš ï¸ GPT ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

        def fetch_gemini():
            try:
                if q_type == "calculation":
                    ans, _ = run_calculation_chain(question, "gemini", st.session_state.vector_db)
                else:
                    ans = run_rag_final(question, answer_style, "gemini", gem_h, shared_docs)
                
                refs = set([f"- {d.metadata['source']} p.{d.metadata['page'] + 1}" for d in shared_docs])
                return f"{ans}\n\n---\n**ì°¸ê³ :**\n" + "\n".join(sorted(refs))
            except Exception as e:
                return f"âš ï¸ Gemini ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

        # 4. ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future_gpt = executor.submit(fetch_gpt)
            future_gemini = executor.submit(fetch_gemini)

            with col1:
                with st.chat_message("assistant", avatar="ğŸ¤–"):
                    st.subheader("GPT-5.2")
                    p_gpt = st.empty()
                    p_gpt.info("GPT ë¶„ì„ ì¤‘...")
            
            with col2:
                with st.chat_message("assistant", avatar="â™Š"):
                    st.subheader("Gemini 2.5")
                    p_gem = st.empty()
                    p_gem.info("Gemini ë¶„ì„ ì¤‘...")

            # ê²°ê³¼ ìˆ˜ì§‘
            final_gpt = future_gpt.result()
            final_gemini = future_gemini.result()

            # í™”ë©´ ì—…ë°ì´íŠ¸ ë° ì €ì¥
            p_gpt.markdown(final_gpt)
            st.session_state.gpt_messages.append({"role": "assistant", "content": final_gpt})
            
            p_gem.markdown(final_gemini)
            st.session_state.gemini_messages.append({"role": "assistant", "content": final_gemini})