import streamlit as st
import os
import tempfile
from dotenv import load_dotenv

# 1. í•µì‹¬ ì„¤ê³„ë„ (Core)
from langchain_core.prompts import PromptTemplate

# 2. êµ¬ê¸€ AI ì—°ê²° (Google GenAI)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


# 3. ë°ì´í„°ë² ì´ìŠ¤ (Community)
from langchain_community.vectorstores import FAISS

# 4. ë©”ëª¨ë¦¬ (ê°€ì¥ ì•ˆì „í•œ ìµœì‹  ê²½ë¡œë¡œ ë³€ê²½)
# ë§Œì•½ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ from langchain_community.chat_message_histories import ... ë¡œ ì„ íšŒí•´ì•¼ í•©ë‹ˆë‹¤.
#try:
#    from langchain.memory import ConversationBufferMemory
#except ImportError:
#    from langchain_community.memory import ConversationBufferMemory

# 5. ê²€ìƒ‰ ì—”ì§„ (Classic)
from langchain_classic.chains import RetrievalQA

# 6. ì§ì ‘ ë§Œë“  ëª¨ë“ˆ
from extract_text import extract_documents_from_pdf, split_documents

# --------------------------------
# ê¸°ë³¸ ì„¤ì •
# --------------------------------
load_dotenv()

st.set_page_config(
    page_title="HUFS AI Tutor",
    layout="wide"
)

st.title("HUFS RAG ê¸°ë°˜ AI íŠœí„°")
st.caption("ê°•ì˜ ìë£Œ ê¸°ë°˜ìœ¼ë¡œ Geminiì™€ GPTë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ë©° ì¶œì²˜ë¥¼ ëª…í™•íˆ ì œì‹œí•©ë‹ˆë‹¤.")

# --------------------------------
# ì„¸ì…˜ ìƒíƒœ
# --------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []


if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# --------------------------------
# ì§ˆë¬¸ ë¶„ë¥˜ê¸°
# --------------------------------
def classify_question(question: str) -> str:
    # í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” ì„¤ì •ì´ ë³µì¡í•œ Gemini ëŒ€ì‹  GPT-4o-minië¥¼ ì”ë‹ˆë‹¤. (ë§¤ìš° ì €ë ´)
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì„ 'concept', 'calculation', 'summary' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´. í•œ ë‹¨ì–´ë§Œ ë‹µí•´. ì§ˆë¬¸: {question}"
    result = llm.invoke(prompt)
    return result.content.strip().lower()


# --------------------------------
# ê³„ì‚° ë¬¸ì œ ì „ìš© ì²´ì¸
# --------------------------------
def run_calculation_chain(question: str):
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0
    )

    docs = st.session_state.vector_db.similarity_search(question, k=7  )
    context = "\n\n".join([d.page_content for d in docs])

    template = """
ë„ˆëŠ” ëŒ€í•™ ê³¼ëª© ê³„ì‚° ë¬¸ì œë¥¼ í‘¸ëŠ” ì¡°êµì´ë‹¤.

[ê·œì¹™]
1. í’€ì´ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ì„¤ëª…í•˜ë¼.
2. ìˆ˜ì‹ì„ ëª…í™•íˆ ì œì‹œí•˜ë¼.
3. ë§ˆì§€ë§‰ì— ìµœì¢… ë‹µì„ ì •ë¦¬í•˜ë¼.
4. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆë¼.

[ë¬¸ë§¥]
{context}

[ë¬¸ì œ]
{question}

[í’€ì´]
"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=template
    )

    response = llm.invoke(
        prompt.format(
            context=context,
            question=question
        )
    )

    return response.content, docs

# --------------------------------
# ì¼ë°˜ RAG ì²´ì¸
# --------------------------------
# 2. ë©”ì¸ ë‹µë³€ ì²´ì¸ (GPT-4o ì‚¬ìš© - ì •ë°€í•œ ë…¼ë¦¬)
def run_rag(question: str, answer_style: str):
    # ë‹µë³€ì€ ë” ë˜‘ë˜‘í•œ GPT-4oê°€ ë‹´ë‹¹
    llm = ChatOpenAI(model="gpt-4o", temperature=0.7)

    retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": 7})
    docs = retriever.invoke(question)

    context = "\n\n".join([d.page_content for d in docs])

    chat_history = "\n".join(
        [f"{m['role']}: {m['content']}" for m in st.session_state.messages]
    )

    length_instruction = (
        "í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ë¼."
        if answer_style == "ì§§ê²Œ"
        else
        "ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìì„¸íˆ ì„¤ëª…í•˜ë¼."
    )

    prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµì˜ 1íƒ€ ê°•ì‚¬ AI íŠœí„°ì…ë‹ˆë‹¤. 
ì œê³µëœ [ê°•ì˜ ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ê·œì¹™]
1. ë°˜ë“œì‹œ ë¬¸ë§¥ì— ê·¼ê±°í•´ ë‹µí•˜ë¼.
2. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
3. ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œë¥¼ ëª…ì‹œí•˜ë¼.
4. ë‹µë³€ì€ ìµœì†Œ 3ë¬¸ë‹¨ ì´ìƒì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.
5. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ê°œë…ì„ ë¨¼ì € ì •ì˜í•˜ê³  ìƒì„¸ ì„¤ëª…ì„ ì´ì–´ê°ˆ ê²ƒ.
6. ê°•ì˜ ìë£Œì— ìˆëŠ” ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•  ê²ƒ.
7. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµì„ ë•ê¸° ìœ„í•´ 'ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê°œë…'ì„ í•œ ë¬¸ì¥ ë§ë¶™ì¼ ê²ƒ.
8. {length_instruction}

[ì´ì „ ëŒ€í™”]
{chat_history}

[ë¬¸ë§¥]
{context}

[ì§ˆë¬¸]
{question}

ë‹µë³€ (ì „ë¬¸ì ì´ê³  ìƒì„¸í•˜ê²Œ):
"""

    response = llm.invoke(prompt)
    return response.content, docs


# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
with st.sidebar:
    st.header("ì„¤ì •")

    answer_style = st.radio(
        "ë‹µë³€ ê¸¸ì´",
        ["ì§§ê²Œ", "ìì„¸íˆ"],
        index=1
    )

    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

    st.divider()

    uploaded_files = st.file_uploader(
        "PDF ì—…ë¡œë“œ",
        type="pdf",
        accept_multiple_files=True
    )

    if uploaded_files and st.button("í•™ìŠµ ì‹œì‘"):
        # --- ì—¬ê¸°ì„œë¶€í„° ìˆ˜ì • (ì§„í–‰ ë°” ë° í…ìŠ¤íŠ¸ ê³µê°„ í™•ë³´) ---
        status_placeholder = st.empty()
        progress_bar = st.progress(0)
        
        with st.spinner("ìë£Œ ë¶„ì„ ì¤‘..."):
            all_docs = []

            for i, file in enumerate(uploaded_files):
                with tempfile.NamedTemporaryFile(delete=False) as tmp:
                    tmp.write(file.getvalue())
                    tmp_path = tmp.name

                # ğŸ§ í˜„ì¬ ì–´ë–¤ íŒŒì¼ì„ ë¶„ì„ ì¤‘ì¸ì§€ í‘œì‹œ
                status_placeholder.info(f"ğŸ“„ '{file.name}' ë¶„ì„ ì¤‘... (íŒŒì¼ {i+1}/{len(uploaded_files)})")
                
                # í•˜ì´ë¸Œë¦¬ë“œ OCR í•¨ìˆ˜ í˜¸ì¶œ
                docs = extract_documents_from_pdf(
                    tmp_path,
                    source_name=file.name
                )
                all_docs.extend(docs)
                os.remove(tmp_path)
                
                # íŒŒì¼ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress(int((i + 1) / len(uploaded_files) * 50))

            status_placeholder.info("ğŸ§  ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘... (ê±°ì˜ ë‹¤ ëì–´ìš”!)")
            
            chunks = split_documents(all_docs)
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001"
            )

            st.session_state.vector_db = FAISS.from_documents(
                chunks, embedding=embeddings
            )  

            # ëª¨ë“  ê³¼ì • ì™„ë£Œ ì²˜ë¦¬
            progress_bar.progress(100)
            status_placeholder.success(f"âœ… ì´ {len(uploaded_files)}ê°œì˜ íŒŒì¼ í•™ìŠµ ì™„ë£Œ!")
            # --- ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • ---

# --------------------------------
# ì±„íŒ… UI
# --------------------------------
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    if st.session_state.vector_db is None:
        st.warning("ë¨¼ì € PDFë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
    else:
        st.session_state.messages.append(
            {"role": "user", "content": question}
        )

        with st.chat_message("user"):
            st.markdown(question)

        q_type = classify_question(question)

        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                if q_type == "calculation":
                    answer, sources = run_calculation_chain(question)
                else:
                    answer, sources = run_rag(question, answer_style)


                refs = set()
                for d in sources:
                    refs.add(
                        f"- {d.metadata['source']} p.{d.metadata['page'] + 1}"
                    )

                final_answer = (
                    f"{answer}\n\n---\n"
                    f"ì°¸ê³  ìë£Œ:\n" + "\n".join(sorted(refs))
                )

                st.markdown(final_answer)

                st.session_state.messages.append(
                    {"role": "assistant", "content": final_answer}
                )
