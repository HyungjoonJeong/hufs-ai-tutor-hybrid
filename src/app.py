import streamlit as st
import os
import tempfile
import concurrent.futures # íŒŒì¼ ìµœìƒë‹¨ì— ì¶”ê°€í•´ì£¼ì„¸ìš”!
from dotenv import load_dotenv

# 1. í•µì‹¬ ì„¤ê³„ë„ (Core)
from langchain_core.prompts import PromptTemplate

# 2. êµ¬ê¸€ AI ì—°ê²° (Google GenAI)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


# 3. ë°ì´í„°ë² ì´ìŠ¤ (Community)
from langchain_community.vectorstores import FAISS

# 4. ë©”ëª¨ë¦¬ (ê°€ì¥ ì•ˆì „í•œ ìµœì‹  ê²½ë¡œë¡œ ë³€ê²½)
# ë§Œì•½ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ from langchain_community.chat_message_histories import ... ë¡œ ì„ íšŒí•´ì•¼ í•©ë‹ˆë‹¤.
#try:
#    from langchain.memory import ConversationBufferMemory
#except ImportError:
#    from langchain_community.memory import ConversationBufferMemory

# 5. ê²€ìƒ‰ ì—”ì§„ (Classic)
from langchain_classic.chains import RetrievalQA

# 6. ì§ì ‘ ë§Œë“  ëª¨ë“ˆ
from extract_text import extract_documents_from_pdf, split_documents

# --------------------------------
# ê¸°ë³¸ ì„¤ì •
# --------------------------------
load_dotenv()

st.set_page_config(
    page_title="HUFS AI Tutor",
    layout="wide"
)

st.title("HUFS RAG ê¸°ë°˜ AI íŠœí„° (Gemini 2.5 & GPT-5.2)")
st.caption("ê°•ì˜ ìë£Œ ê¸°ë°˜ìœ¼ë¡œ Geminiì™€ GPTë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ë©° ì¶œì²˜ë¥¼ ëª…í™•íˆ ì œì‹œí•©ë‹ˆë‹¤.")

# --------------------------------
# ì„¸ì…˜ ìƒíƒœ
# --------------------------------
# --- app.py ìƒë‹¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë¶€ë¶„ ---

if "gpt_messages" not in st.session_state:
    st.session_state.gpt_messages = []

if "gemini_messages" not in st.session_state:
    st.session_state.gemini_messages = []

# ê¸°ì¡´ messagesëŠ” ë” ì´ìƒ ì“°ì§€ ì•Šì§€ë§Œ, 
# í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‚¨ê²¨ë‘ê±°ë‚˜ ì•„ë˜ì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
# if "messages" not in st.session_state:
#     st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# --------------------------------
# ì§ˆë¬¸ ë¶„ë¥˜ê¸°
# --------------------------------
def classify_question(question: str) -> str:
    # í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” ì„¤ì •ì´ ë³µì¡í•œ Gemini ëŒ€ì‹  GPT-4o-minië¥¼ ì”ë‹ˆë‹¤. (ë§¤ìš° ì €ë ´)
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì„ 'concept', 'calculation', 'summary' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´. í•œ ë‹¨ì–´ë§Œ ë‹µí•´. ì§ˆë¬¸: {question}"
    result = llm.invoke(prompt)
    return result.content.strip().lower()


# --------------------------------
# --------------------------------
# ê³„ì‚° ë¬¸ì œ ì „ìš© ì²´ì¸ (GPT/Gemini ëŒ€ì‘)
# --------------------------------
def run_calculation_chain(question: str, model_type: str = "gemini"):
    # 1. ëª¨ë¸ ì„ íƒ
    if model_type == "gpt":
        llm = ChatOpenAI(model="gpt-5.2", temperature=0)
    else:
        # 2026ë…„ ê¸°ì¤€ ìµœì‹  ì•ˆì • ë²„ì „ì¸ 1.5-flash ê¶Œì¥
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

    # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = st.session_state.vector_db.similarity_search(question, k=7)
    context = "\n\n".join([d.page_content for d in docs])

    template = """
ë„ˆëŠ” ëŒ€í•™ ê³¼ëª© ê³„ì‚° ë¬¸ì œë¥¼ í‘¸ëŠ” ì¡°êµì´ë‹¤. ì œê³µëœ [ë¬¸ë§¥]ì˜ ê³µì‹ê³¼ ìˆ˜ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ì–´ë¼.

[ê·œì¹™]
1. í’€ì´ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ìƒì„¸íˆ ì„¤ëª…í•˜ë¼.
2. ìˆ˜ì‹ì€ LaTeX í˜•ì‹ì´ë‚˜ ëª…í™•í•œ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œì‹œí•˜ë¼.
3. ë§ˆì§€ë§‰ì— ìµœì¢… ë‹µì„ 'ì •ë‹µ: 'ê³¼ í•¨ê»˜ ì •ë¦¬í•˜ë¼.
4. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ê°€ê¸‰ì  ì‚¬ìš©í•˜ì§€ ë§ê³ , ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ë¼ê³  ì•ˆë‚´í•˜ë¼.

[ë¬¸ë§¥]
{context}

[ë¬¸ì œ]
{question}

[í’€ì´]
"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=template
    )

    # 3. ë‹µë³€ ìƒì„±
    response = llm.invoke(
        prompt.format(
            context=context,
            question=question
        )
    )

    return response.content, docs
# --------------------------------
# ì¼ë°˜ RAG ì²´ì¸
# --------------------------------
# 2. ë©”ì¸ ë‹µë³€ ì²´ì¸ (GPT-4o ì‚¬ìš© - ì •ë°€í•œ ë…¼ë¦¬)
# run_rag ì •ì˜ ë¶€ë¶„ ìˆ˜ì •
def run_rag(question: str, answer_style: str, model_type: str = "gpt"):
    # 1. ëª¨ë¸ ë° íˆìŠ¤í† ë¦¬ ì„¤ì •
    if model_type == "gpt":
        llm = ChatOpenAI(model="gpt-5.2", temperature=0.7)
        history = st.session_state.gpt_messages[:-1]
    else:
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)
        history = st.session_state.gemini_messages[:-1]

    # 2. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": 7})
    docs = retriever.invoke(question)
    context_text = "\n\n".join([d.page_content for d in docs])

    chat_history_str = "\n".join([f"{m['role']}: {m['content']}" for m in history])

    length_instruction = (
        "í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ë¼." if answer_style == "ì§§ê²Œ" 
        else "ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìì„¸íˆ ì„¤ëª…í•˜ë¼."
    )

    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (f-string ëŒ€ì‹  ì¼ë°˜ ë¬¸ìì—´ê³¼ .format() ì¶”ì²œ)
    template = """
ë‹¹ì‹ ì€ í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµì˜ 1íƒ€ ê°•ì‚¬ AI íŠœí„°ì…ë‹ˆë‹¤. 
ì œê³µëœ [ê°•ì˜ ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ê·œì¹™]
1. ë°˜ë“œì‹œ ë¬¸ë§¥ì— ê·¼ê±°í•´ ë‹µí•˜ë¼.
2. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆë¼.
3. ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œì™€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ë¼.
4. ë‹µë³€ì€ ìµœì†Œ 3ë¬¸ë‹¨ ì´ìƒì˜ ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.
5. ê°•ì˜ ìë£Œì— ìˆëŠ” ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•  ê²ƒ.
6. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµì„ ë•ê¸° ìœ„í•´ 'ê´€ë ¨í•˜ì—¬ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê°œë…'ì„ í•œë‘ ë¬¸ì¥ ë§ë¶™ì¼ ê²ƒ.
7. {length_instruction}

[ì´ì „ ëŒ€í™”]
{chat_history}

[ë¬¸ë§¥]
{context}

[ì§ˆë¬¸]
{question}

ë‹µë³€ (ì „ë¬¸ì ì´ê³  ìƒì„¸í•˜ê²Œ):
"""

# ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ .format() ì‚¬ìš©
    prompt = template.format(
        length_instruction=length_instruction,
        chat_history=chat_history_str,
        context=context_text,
        question=question
    )

    response = llm.invoke(prompt)
    return response.content, docs



# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
# --------------------------------
# ì‚¬ì´ë“œë°”
# --------------------------------
with st.sidebar:
    st.header("ì„¤ì •")

    answer_style = st.radio(
        "ë‹µë³€ ê¸¸ì´",
        ["ì§§ê²Œ", "ìì„¸íˆ"],
        index=1
    )

    # ì‚¬ì´ë“œë°”ì˜ ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ë¶€ë¶„
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.gpt_messages = []
        st.session_state.gemini_messages = []
        st.rerun()

    st.divider()

    uploaded_files = st.file_uploader(
        "PDF ì—…ë¡œë“œ",
        type="pdf",
        accept_multiple_files=True
    )

    if uploaded_files and st.button("í•™ìŠµ ì‹œì‘"):
        # --- ì—¬ê¸°ì„œë¶€í„° ìˆ˜ì • (ì§„í–‰ ë°” ë° í…ìŠ¤íŠ¸ ê³µê°„ í™•ë³´) ---
        status_placeholder = st.empty()
        progress_bar = st.progress(0)
        
        with st.spinner("ìë£Œ ë¶„ì„ ì¤‘..."):
            all_docs = []

            for i, file in enumerate(uploaded_files):
                with tempfile.NamedTemporaryFile(delete=False) as tmp:
                    tmp.write(file.getvalue())
                    tmp_path = tmp.name

                # ğŸ§ í˜„ì¬ ì–´ë–¤ íŒŒì¼ì„ ë¶„ì„ ì¤‘ì¸ì§€ í‘œì‹œ
                status_placeholder.info(f"ğŸ“„ '{file.name}' ë¶„ì„ ì¤‘... (íŒŒì¼ {i+1}/{len(uploaded_files)})")
                
                # í•˜ì´ë¸Œë¦¬ë“œ OCR í•¨ìˆ˜ í˜¸ì¶œ
                docs = extract_documents_from_pdf(
                    tmp_path,
                    source_name=file.name
                )
                all_docs.extend(docs)
                os.remove(tmp_path)
                
                # íŒŒì¼ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress(int((i + 1) / len(uploaded_files) * 50))

            status_placeholder.info("ğŸ§  ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘... (ê±°ì˜ ë‹¤ ëì–´ìš”!)")
            
            chunks = split_documents(all_docs)
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001"
            )

            st.session_state.vector_db = FAISS.from_documents(
                chunks, embedding=embeddings
            )  

            # ëª¨ë“  ê³¼ì • ì™„ë£Œ ì²˜ë¦¬
            progress_bar.progress(100)
            status_placeholder.success(f"âœ… ì´ {len(uploaded_files)}ê°œì˜ íŒŒì¼ í•™ìŠµ ì™„ë£Œ!")
            # --- ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • ---

# --------------------------------
# --------------------------------
# ì±„íŒ… UI (ì´ì „ ëŒ€í™” ê¸°ë¡ ë³µì›)
# --------------------------------
# í™”ë©´ì„ 2ê°œë¡œ ë‚˜ëˆ ì„œ ê° ëª¨ë¸ì˜ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¢Œìš°ì— ë°°ì¹˜í•©ë‹ˆë‹¤.
view_col1, view_col2 = st.columns(2)

with view_col1:
    for msg in st.session_state.gpt_messages:
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ GPTì˜ ë‹µë³€ì„ ì°¨ë¡€ë¡œ ì¶œë ¥
        avatar = "ğŸ¤–" if msg["role"] == "assistant" else None
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

with view_col2:
    for msg in st.session_state.gemini_messages:
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ Geminiì˜ ë‹µë³€ì„ ì°¨ë¡€ë¡œ ì¶œë ¥
        avatar = "â™Š" if msg["role"] == "assistant" else None
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

# --------------------------------
# ì‹ ê·œ ì§ˆë¬¸ ì…ë ¥ ë° ì²˜ë¦¬
# --------------------------------

if question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    if st.session_state.vector_db is None:
        st.warning("ë¨¼ì € PDFë¥¼ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
    else:
        # 1. ê³µí†µ ì§ˆë¬¸ ì €ì¥
        st.session_state.gpt_messages.append({"role": "user", "content": question})
        st.session_state.gemini_messages.append({"role": "user", "content": question})

        # 2. ì‚¬ìš©ì ì§ˆë¬¸ í™”ë©´ ì¶œë ¥
        with st.chat_message("user"):
            st.markdown(question)

        # 3. ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        q_type = classify_question(question)

        # 4. ì¢Œìš° ì»¬ëŸ¼ ìƒì„±
        col1, col2 = st.columns(2)
        
        # --- ë³‘ë ¬ ì‹¤í–‰ ë¡œì§ ì‹œì‘ ---
        # ë‘ ëª¨ë¸ì„ ë™ì‹œì— í˜¸ì¶œí•˜ê¸° ìœ„í•œ 'ì‘ì—…ì‹¤'ì„ ë§Œë“­ë‹ˆë‹¤.
        def get_gpt_answer():
            if q_type == "calculation":
                ans, src = run_calculation_chain(question, model_type="gpt")
            else:
                ans, src = run_rag(question, answer_style, model_type="gpt")
            refs = set([f"- {d.metadata['source']} p.{d.metadata['page'] + 1}" for d in src])
            return f"{ans}\n\n---\n**ì°¸ê³ :**\n" + "\n".join(sorted(refs))

        def get_gemini_answer():
            if q_type == "calculation":
                ans, src = run_calculation_chain(question, model_type="gemini")
            else:
                ans, src = run_rag(question, answer_style, model_type="gemini")
            refs = set([f"- {d.metadata['source']} p.{d.metadata['page'] + 1}" for d in src])
            return f"{ans}\n\n---\n**ì°¸ê³ :**\n" + "\n".join(sorted(refs))

        # ë™ì‹œì— ì‹¤í–‰!
        with concurrent.futures.ThreadPoolExecutor() as executor:
            # ë‘ ì‘ì—…ì„ ìŠ¤ë ˆë“œ í’€ì— ë˜ì§‘ë‹ˆë‹¤.
            future_gpt = executor.submit(get_gpt_answer)
            future_gemini = executor.submit(get_gemini_answer)

            # í™”ë©´ì—ëŠ” ë™ì‹œì— ë±…ê¸€ë±…ê¸€(Spinner)ì„ ë„ì›ë‹ˆë‹¤.
            with col1:
                with st.chat_message("assistant", avatar="ğŸ¤–"):
                    st.subheader("GPT-5.2")
                    placeholder_gpt = st.empty()
                    with placeholder_gpt:
                        st.spinner("GPT ë¶„ì„ ì¤‘...")
            
            with col2:
                with st.chat_message("assistant", avatar="â™Š"):
                    st.subheader("Gemini 2.5")
                    placeholder_gemini = st.empty()
                    with placeholder_gemini:
                        st.spinner("Gemini ë¶„ì„ ì¤‘...")

            # ê²°ê³¼ê°€ ë¨¼ì € ë‚˜ì˜¤ëŠ” ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ í™”ë©´ì— ë¿Œë¦½ë‹ˆë‹¤.
            final_gpt = future_gpt.result()
            final_gemini = future_gemini.result()

            # ê²°ê³¼ í™”ë©´ ì—…ë°ì´íŠ¸ ë° ì„¸ì…˜ ì €ì¥
            with col1:
                placeholder_gpt.markdown(final_gpt)
                st.session_state.gpt_messages.append({"role": "assistant", "content": final_gpt})
            
            with col2:
                placeholder_gemini.markdown(final_gemini)
                st.session_state.gemini_messages.append({"role": "assistant", "content": final_gemini})