import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_classic.chains import RetrievalQA
# ê¸°ì¡´ ì½”ë“œ (ì—ëŸ¬ ë°œìƒ)
# from langchain.prompts import PromptTemplate

# ìˆ˜ì •ëœ ì½”ë“œ (ìµœì‹  ë²„ì „ ê²½ë¡œ)
from langchain_core.prompts import PromptTemplate

load_dotenv()

def run_ai_tutor():
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_db = FAISS.load_local("my_vector_db", embeddings, allow_dangerous_deserialization=True)
    
    # 1. ëª¨ë¸ ì„¤ì • (ìµœì‹  ë²„ì „ ìœ ì§€)
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7) # ì°½ì˜ì„±ì„ ìœ„í•´ 0.7ë¡œ ì¡°ì ˆ

    # 2. íŠœí„° ì „ìš© ì§€ì¹¨(Prompt) ë§Œë“¤ê¸°
    template = """
    ë„ˆëŠ” í•œêµ­ì™¸ëŒ€ í•™ìƒë“¤ì„ ìœ„í•œ ì•„ì£¼ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ 'AI í•™ìŠµ íŠœí„°'ì•¼.
    ì•„ë˜ ì œê³µëœ [ë¬¸ë§¥]ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ [ì§ˆë¬¸]ì— ë‹µë³€í•´ì¤˜.
    
    ë‹µë³€ ì§€ì¹¨:
    1. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ì¤˜.
    2. ëª¨ë¥´ëŠ” ë‚´ìš©ì´ë¼ë©´ ì–µì§€ë¡œ ì§€ë©°ë‚´ì§€ ë§ê³  "ê°•ì˜ ìë£Œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•´ì¤˜.
    3. í•™ìƒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í•µì‹¬ ìš”ì•½ì„ ë¨¼ì € í•´ì£¼ê³ , ìƒì„¸ ì„¤ëª…ì„ ë§ë¶™ì—¬ì¤˜.
    4. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ í•™ìƒì„ ê²©ë ¤í•˜ëŠ” í•œë§ˆë””ë‚˜, "ë” ê¶ê¸ˆí•œ ì ì´ ìˆë‚˜ìš”?"ë¼ëŠ” ì§ˆë¬¸ì„ ë‚¨ê²¨ì¤˜.

    [ë¬¸ë§¥]: {context}
    
    [ì§ˆë¬¸]: {question}
    
    ë‚˜ì˜ ë‹µë³€:"""

    PROMPT = PromptTemplate(
        template=template, input_variables=["context", "question"]
    )

    # 3. ì§ˆë¬¸-ë‹µë³€ ì—”ì§„ ì¡°ë¦½ (í”„ë¡¬í”„íŠ¸ ì¶”ê°€)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_db.as_retriever(),
        chain_type_kwargs={"prompt": PROMPT} # í”„ë¡¬í”„íŠ¸ ì—°ê²°!
    )

    print("\n[ğŸ“ HUFS AI íŠœí„°ê°€ ì¹œì ˆí•œ ëª¨ë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤!]")
    
    while True:
        query = input("\ní•™ìƒ ì§ˆë¬¸: ")
        if query == "ë‚˜ê°€ê¸°":
            break
        
        print("íŠœí„°ê°€ ìë£Œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
        response = qa_chain.invoke(query)
        print(f"\nAI íŠœí„°: {response['result']}")

if __name__ == "__main__":
    run_ai_tutor()