import fitz  # pymupdf
import io
import base64
import streamlit as st
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

def extract_documents_from_pdf(file_path: str, source_name: str):
    vision_model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
    doc = fitz.open(file_path)
    documents = []

    for page_number in range(len(doc)):
        page = doc[page_number]
        
        # 1. í…ìŠ¤íŠ¸ ë ˆì´ì–´ ì¦‰ì‹œ ì¶”ì¶œ
        page_text = page.get_text().strip()
        
        # 2. í˜ì´ì§€ ë‚´ ì´ë¯¸ì§€ ê°ì²´ ì°¾ê¸°
        image_list = page.get_images(full=True)
        image_descriptions = []

        if image_list:
            st.toast(f"ğŸ¨ {page_number + 1}p: ê·¸ë¦¼ {len(image_list)}ê°œ ë¶„ì„ ì¤‘...")
            for img_index, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                
                # Geminiì—ê²Œ ê°œë³„ ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­
                encoded_image = base64.b64encode(image_bytes).decode("utf-8")
                
                image_message = {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{encoded_image}"},
                }
                text_message = {
                    "type": "text",
                    "text": "ì´ ê·¸ë¦¼/ì°¨íŠ¸ê°€ ë¬´ì—‡ì„ ì„¤ëª…í•˜ëŠ”ì§€ í•µì‹¬ë§Œ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜."
                }
                
                try:
                    res = vision_model.invoke([HumanMessage(content=[text_message, image_message])])
                    image_descriptions.append(f"[ê·¸ë¦¼{img_index+1} ì„¤ëª…: {res.content}]")
                except:
                    continue

        # 3. í…ìŠ¤íŠ¸ì™€ ê·¸ë¦¼ ì„¤ëª… ê²°í•©
        full_content = f"{page_text}\n\n" + "\n".join(image_descriptions)
        
        documents.append(
            Document(
                page_content=full_content,
                metadata={"source": source_name, "page": page_number}
            )
        )

    doc.close()
    return documents

def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    return splitter.split_documents(documents)