import fitz  # pymupdf
import io
import base64
import streamlit as st
from PIL import Image
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

def extract_documents_from_pdf(file_path: str, source_name: str):
    # Gemini ëª¨ë¸ ì„¤ì •
    vision_model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
    
    doc = fitz.open(file_path)
    documents = []
    total = len(doc)

    for page_number in range(total):
        page = doc[page_number]
        
        # 1. ë¨¼ì € í•´ë‹¹ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ë´…ë‹ˆë‹¤.
        raw_text = page.get_text().strip()
        
        # [íŒë‹¨ ë¡œì§] 
        # í…ìŠ¤íŠ¸ê°€ 150ì ì´ìƒ í’ë¶€í•˜ê²Œ ìˆë‹¤ë©´ -> ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë“œ (ì´ˆê³ ì†)
        # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ë‹¤ë©´ -> ì´ë¯¸ì§€/ìŠ¤ìº”ë³¸ìœ¼ë¡œ íŒë‹¨í•˜ê³  OCR ëª¨ë“œ (ì •ë°€)
        if len(raw_text) > 150:
            st.toast(f"âš¡ {page_number + 1}p: í…ìŠ¤íŠ¸ ì§ë… ì¤‘...")
            page_content = raw_text
        else:
            st.toast(f"ğŸ‘ï¸ {page_number + 1}p: ì´ë¯¸ì§€ ì •ë°€ ë¶„ì„ ì¤‘...")
            # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
            img_data = pix.tobytes("png")
            encoded_image = base64.b64encode(img_data).decode("utf-8")
            
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{encoded_image}"},
            }
            text_message = {
                "type": "text",
                "text": "ì´ í˜ì´ì§€ëŠ” ì´ë¯¸ì§€ë‚˜ í‘œ ìœ„ì£¼ì…ë‹ˆë‹¤. ë‚´ìš©ì„ ì•„ì£¼ ìƒì„¸í•˜ê²Œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•´ì¤˜."
            }
            
            try:
                response = vision_model.invoke([HumanMessage(content=[text_message, image_message])])
                page_content = response.content
            except Exception as e:
                page_content = f"OCR ì‹¤íŒ¨: {raw_text if raw_text else str(e)}"

        documents.append(
            Document(
                page_content=page_content,
                metadata={"source": source_name, "page": page_number}
            )
        )

    doc.close()
    return documents

def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)
    return splitter.split_documents(documents)