import fitz  # pymupdf
import io
import base64
from PIL import Image
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
import streamlit as st

def extract_documents_from_pdf(file_path: str, source_name: str):
    vision_model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
    doc = fitz.open(file_path)
    documents = []
    total = len(doc)

    for page_number in range(total):
        for page_number in range(total):
        # UI ì—…ë°ì´íŠ¸ìš© (í•¨ìˆ˜ ì•ˆì—ì„œ ìŠ¤íŠ¸ë¦¼ë¦¿ UIë¥¼ ì§ì ‘ ê±´ë“œë¦¼)
            st.toast(f"ğŸ“„ {page_number + 1} / {total} í˜ì´ì§€ ë¶„ì„ ì¤‘...")

        page = doc[page_number]
        text = page.get_text().strip()
        
        # [ì „ëµ] í…ìŠ¤íŠ¸ê°€ ì¼ì •ëŸ‰(ì˜ˆ: 100ì) ì´ìƒ ìˆê³ , ì´ë¯¸ì§€ê°€ ì ìœ¼ë©´ ë°”ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´(ì´ë¯¸ì§€ PDFê±°ë‚˜ í‘œê°€ ë§ìœ¼ë©´) Gemini OCR ê°€ë™
        if len(text) > 100:
            page_content = f"[Text Extraction]\n{text}"
            st.toast(f"âš¡ {page_number + 1}p: í…ìŠ¤íŠ¸ ì§ë… ì¤‘...")
        else:
            st.toast(f"ğŸ‘ï¸ {page_number + 1}p: ì´ë¯¸ì§€ ë¶„ì„(OCR) ì¤‘...")
            # ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³€í™˜
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
            img_data = pix.tobytes("png")
            encoded_image = base64.b64encode(img_data).decode("utf-8")
            
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{encoded_image}"},
            }
            text_message = {
                "type": "text",
                "text": "ì´ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì•„ì£¼ ìƒì„¸í•˜ê²Œ í…ìŠ¤íŠ¸ë¡œ ë³µì›í•´ì¤˜. í‘œëŠ” ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ, ê·¸ë¦¼ì€ ì„¤ëª…ìœ¼ë¡œ í¬í•¨í•´ì¤˜."
            }
            
            try:
                response = vision_model.invoke([HumanMessage(content=[text_message, image_message])])
                page_content = f"[OCR Extraction]\n{response.content}"
            except Exception as e:
                page_content = f"ì—ëŸ¬ ë°œìƒ: {str(e)}"

        documents.append(
            Document(
                page_content=page_content,
                metadata={"source": source_name, "page": page_number}
            )
        )

    doc.close()
    return documents

def split_documents(documents):
    # OCRë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ëŠ” ì •ë³´ ë°€ë„ê°€ ë†’ìœ¼ë¯€ë¡œ chunk í¬ê¸°ë¥¼ ë„‰ë„‰í•˜ê²Œ ì¡ìŠµë‹ˆë‹¤.
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=200
    )
    return splitter.split_documents(documents)