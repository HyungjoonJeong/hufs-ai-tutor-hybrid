import fitz  # pymupdf
import io
import base64
import streamlit as st
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

def extract_documents_from_pdf(file_path: str, source_name: str):
    vision_model = ChatGoogleGenerativeAI(model="gemini-1.5-flash")
    doc = fitz.open(file_path)
    documents = []

    for page_number in range(len(doc)):
        page = doc[page_number]
        
        # 1. í…ìŠ¤íŠ¸ëŠ” ì›ë³¸ì—ì„œ ì¦‰ì‹œ ì¶”ì¶œ
        page_text = page.get_text().strip()
        
        # 2. ê·¸ë¦¼ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if page.get_images(full=True):
            st.toast(f"ğŸ–¼ï¸ {page_number + 1}p: ì‹œê° ìë£Œ ì¶”ì¶œ ì¤‘...")
            
            # í…ìŠ¤íŠ¸ ì˜ì—­ì„ ëª¨ë‘ ì°¾ì•„ì„œ í°ìƒ‰ìœ¼ë¡œ ê°€ë¦½ë‹ˆë‹¤ (Redact)
            for text_instance in page.search_for(" "): # ëª¨ë“  ê³µë°±/ë¬¸ì íƒìƒ‰
                page.add_redact_annot(text_instance, fill=(1, 1, 1)) # í°ìƒ‰ ì±„ìš°ê¸°
            page.apply_redactions() # ê°€ë¦¬ê¸° ì ìš©
            
            # ì´ì œ í…ìŠ¤íŠ¸ê°€ ì‚¬ë¼ì§„ 'ê·¸ë¦¼ë§Œ ë‚¨ì€ í˜ì´ì§€'ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            pix = page.get_pixmap(matrix=fitz.Matrix(1.5, 1.5))
            img_data = base64.b64encode(pix.tobytes("png")).decode("utf-8")
            
            # Geminiì—ê²Œ ìˆœìˆ˜í•˜ê²Œ ì‹œê° ì •ë³´ë§Œ ë¶„ì„ ìš”ì²­
            message = HumanMessage(
                content=[
                    {"type": "text", "text": "ì´ ì´ë¯¸ì§€ì—ì„œ ê¸€ìëŠ” ë¬´ì‹œí•˜ê³ , ê·¸ë¦¼ì´ë‚˜ ë„í‘œê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ë¶„ì„í•´ì¤˜."},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_data}"}}
                ]
            )
            
            try:
                res = vision_model.invoke([message])
                page_text += f"\n\n[ì‹œê° ìë£Œ ë¶„ì„]\n{res.content}"
            except:
                pass

        documents.append(
            Document(
                page_content=page_text,
                metadata={"source": source_name, "page": page_number}
            )
        )

    doc.close()
    return documents

def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    return splitter.split_documents(documents)